{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLfnujch4uHWxpwhmhNmDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naomy-Yailin/SIS420/blob/main/laboratorio3/reg_log_onevsall_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Este conjunto de datos consta de 101 animales de un zool√≥gico.\n",
        "# Hay 16 variables con varios rasgos para describir a los animales. **texto en negrita**\n",
        "# Los 7 tipos de clase son: Mam√≠fero, Ave, Reptil, Pez, Anfibio, Insecto e Invertebrado **texto en negrita**\n",
        "\n",
        "# El prop√≥sito de este conjunto de datos es poder predecir la clasificaci√≥n de los animales, en funci√≥n de las variables. **texto en negrita**"
      ],
      "metadata": {
        "id": "nMeQ-YxWBufv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDH3MmzOwUST",
        "outputId": "7ee84236-c1a3-4418-a017-2f3942eef6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilizado para la manipulaci√≥n de directorios y rutas\n",
        "import os\n",
        "\n",
        "# C√°lculo cient√≠fico y vectorial para python\n",
        "import numpy as np\n",
        "\n",
        "# Libreria para graficos\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Modulo de optimizacion en scipy\n",
        "from scipy import optimize\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# modulo para cargar archivos en formato MATLAB\n",
        "# from scipy.io import loadmat\n",
        "\n",
        "# le dice a matplotlib que incruste gr√°ficos en el cuaderno\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "t9oIAtCfwgwe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar dataset Zoo\n",
        "data = pd.read_csv('/content/zoo.csv')\n",
        "\n",
        "# Extraer X (caracter√≠sticas) y y (etiquetas)\n",
        "X = data.iloc[:, 1:-1].values   # todas las columnas menos 'animal_name' y 'class_type'\n",
        "y = data.iloc[:, -1].values     # columna class_type\n",
        "\n",
        "# Definir n√∫mero de entradas y salidas\n",
        "# Hay 16 atributos en el dataset, pero el comentario original contaba x0 (bias)\n",
        "input_layer_size = X.shape[1] + 1   # 16 + 1 = 17\n",
        "num_labels = len(np.unique(y))      # 7 clases distintas\n",
        "\n",
        "# Ajustar etiquetas: pasar de 1-7 a 0-6 para evitar problemas\n",
        "y = y - 1\n",
        "\n",
        "# N√∫mero de ejemplos de entrenamiento\n",
        "m = y.size\n",
        "\n",
        "print(\"N√∫mero de ejemplos:\", m)\n",
        "print(\"Dimensi√≥n de X:\", X.shape)\n",
        "print(\"Clases distintas:\", num_labels)\n",
        "print(\"Primeras etiquetas:\", y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4ouLimQ3xIe",
        "outputId": "5cc60a5a-20cb-4e56-b87d-12872e9f8456"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N√∫mero de ejemplos: 101\n",
            "Dimensi√≥n de X: (101, 16)\n",
            "Clases distintas: 7\n",
            "Primeras etiquetas: [0 0 3 0 0 0 0 3 3 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0,:])\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob-4Vvly6vNV",
        "outputId": "3409427d-a512-4c20-d5ca-09bb24f3a99b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0 0 1 1 1 1 0 0 4 0 0 1]\n",
            "[0 0 3 0 0 0 0 3 3 0 0 1 3 6 6 6 1 0 3 0 1 1 0 1 5 4 4 0 0 0 5 0 0 1 3 0 0\n",
            " 1 3 5 5 1 5 1 0 0 6 0 0 0 0 5 4 6 0 0 1 1 1 1 3 3 2 0 0 0 0 0 0 0 0 1 6 3\n",
            " 0 0 2 6 1 1 2 6 3 1 0 6 3 1 5 4 2 2 3 0 0 1 0 5 0 6 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def featureNormalize(X):\n",
        "    mu = np.mean(X, axis=0)\n",
        "    sigma = np.std(X, axis=0, ddof=1)  # ddof=1 para desviaci√≥n est√°ndar muestral\n",
        "    X_norm = (X - mu) / sigma\n",
        "    return X_norm, mu, sigma\n",
        "\n",
        "# Aplicar a tu dataset zoo\n",
        "X_norm, mu, sigma = featureNormalize(X)\n",
        "\n",
        "print(\"Primer ejemplo normalizado:\", X_norm[0,:])\n",
        "print(\"Medias:\", mu)\n",
        "print(\"Desviaciones est√°ndar:\", sigma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyFonvCD7M-v",
        "outputId": "1120640c-fea4-4aed-edc2-14947e3d98d1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primer ejemplo normalizado: [ 1.15563073 -0.49443795 -1.17934447  1.20371316 -0.55551984 -0.74051504\n",
            "  0.89197269  0.80575756  0.46337918  0.50980485 -0.29183867 -0.4476351\n",
            "  0.56969831 -1.68998664 -0.38244559  1.13253179]\n",
            "Medias: [0.42574257 0.1980198  0.58415842 0.40594059 0.23762376 0.35643564\n",
            " 0.55445545 0.6039604  0.82178218 0.79207921 0.07920792 0.16831683\n",
            " 2.84158416 0.74257426 0.12871287 0.43564356]\n",
            "Desviaciones est√°ndar: [0.49692121 0.40049474 0.49532468 0.4935224  0.42775027 0.48133478\n",
            " 0.49950471 0.49151211 0.38460472 0.40784388 0.27140996 0.37601348\n",
            " 2.03338473 0.43939653 0.33655212 0.49831399]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llama featureNormalize con los datos cargados\n",
        "X_norm, mu, sigma = featureNormalize(X)"
      ],
      "metadata": {
        "id": "YCQXQNCU7TCc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_norm[0,:])\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8dciwRO7luC",
        "outputId": "60d568ef-6d00-4301-d66d-2c8771bf619b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.15563073 -0.49443795 -1.17934447  1.20371316 -0.55551984 -0.74051504\n",
            "  0.89197269  0.80575756  0.46337918  0.50980485 -0.29183867 -0.4476351\n",
            "  0.56969831 -1.68998664 -0.38244559  1.13253179]\n",
            "[0 0 3 0 0 0 0 3 3 0 0 1 3 6 6 6 1 0 3 0 1 1 0 1 5 4 4 0 0 0 5 0 0 1 3 0 0\n",
            " 1 3 5 5 1 5 1 0 0 6 0 0 0 0 5 4 6 0 0 1 1 1 1 3 3 2 0 0 0 0 0 0 0 0 1 6 3\n",
            " 0 0 2 6 1 1 2 6 3 1 0 6 3 1 5 4 2 2 3 0 0 1 0 5 0 6 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la matriz adecuadamente, y agregar una columna de unos que corresponde al termino de intercepci√≥n.\n",
        "m, n = X.shape\n",
        "# Agraga el termino de intercepci√≥n a A\n",
        "# X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)\n",
        "X = X_norm\n",
        "# X = np.concatenate([np.ones((m, 1)), X], axis=1)"
      ],
      "metadata": {
        "id": "sNh64zhD7xge"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Vectorizaci√≥n de regresi√≥n log√≠stica\n",
        "\n",
        "Se utilizar√° m√∫ltiples modelos de regresi√≥n log√≠stica uno contra todos para construir un clasificador de clases m√∫ltiples. Dado que hay 10 clases, deber√° entrenar 10 clasificadores de regresi√≥n log√≠stica separados. Para que esta capacitaci√≥n sea eficiente, es importante asegurarse de que el c√≥digo est√© bien vectorizado.\n",
        "\n",
        "En esta secci√≥n, se implementar√° una versi√≥n vectorizada de regresi√≥n log√≠stica que no emplea ning√∫n bucle \"for\".\n",
        "\n",
        "Para probar la regresi√≥n log√≠stica vectorizada, se usara datos personalizados como se definen a continuaci√≥n."
      ],
      "metadata": {
        "id": "I8OxTQj1BR9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"section1\"></a>\n",
        "#### 1.3.1 Vectorizaci√≥n de la funcion de costo\n",
        "\n",
        "Se inicia escribiendo una versi√≥n vectorizada de la funci√≥n de costo. En la regresi√≥n log√≠stica (no regularizada), la funci√≥n de costo es\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left( h_\\theta\\left( x^{(i)} \\right) \\right) - \\left(1 - y^{(i)} \\right) \\log \\left(1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] $$\n",
        "\n",
        "Para calcular cada elemento en la suma, tenemos que calcular $h_\\theta(x^{(i)})$ para cada ejemplo $i$, donde $h_\\theta(x^{(i)}) = g(\\theta^T x^{(i)})$ y $g(z) = \\frac{1}{1+e^{-z}}$ es la funcion sigmoidea. Resulta que podemos calcular esto r√°pidamente para todos los ejemplos usando la multiplicaci√≥n de matrices. Definamos $X$ y $\\theta$ como\n",
        "\n",
        "$$ X = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T - \\\\ - \\left( x^{(2)} \\right)^T - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T - \\end{bmatrix} \\qquad \\text{and} \\qquad \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} $$\n",
        "\n",
        "Luego, de calcular el producto matricial $X\\theta$, se tiene:\n",
        "\n",
        "$$ X\\theta = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T\\theta - \\\\ - \\left( x^{(2)} \\right)^T\\theta - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T\\theta - \\end{bmatrix} = \\begin{bmatrix} - \\theta^T x^{(1)}  - \\\\ - \\theta^T x^{(2)} - \\\\ \\vdots \\\\ - \\theta^T x^{(m)}  - \\end{bmatrix} $$\n",
        "\n",
        "En la √∫ltima igualdad, usamos el hecho de que $a^Tb = b^Ta$ if $a$ y $b$ son vectores. Esto permite calcular los productos $\\theta^T x^{(i)}$ para todos los ejemplos $i$ en una linea de codigo.\n",
        "\n",
        "#### 1.3.2 Vectorizaci√≥n del gradiente\n",
        "\n",
        "Recordemos que el gradiente del costo de regresi√≥n log√≠stica (no regularizado) es un vector donde el elemento $j^{th}$ se define como\n",
        "$$ \\frac{\\partial J }{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\left( h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_j^{(i)} \\right) $$\n",
        "\n",
        "Para vectorizar esta operaci√≥n sobre el conjunto de datos, se inicia escribiendo todas las derivadas parciales expl√≠citamente para todos $\\theta_j$,\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial J}{\\partial \\theta_0} \\\\\n",
        "\\frac{\\partial J}{\\partial \\theta_1} \\\\\n",
        "\\frac{\\partial J}{\\partial \\theta_2} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial J}{\\partial \\theta_n}\n",
        "\\end{bmatrix} = &\n",
        "\\frac{1}{m} \\begin{bmatrix}\n",
        "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_0^{(i)}\\right) \\\\\n",
        "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_1^{(i)}\\right) \\\\\n",
        "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_2^{(i)}\\right) \\\\\n",
        "\\vdots \\\\\n",
        "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_n^{(i)}\\right) \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "= & \\frac{1}{m} \\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x^{(i)}\\right) \\\\\n",
        "= & \\frac{1}{m} X^T \\left( h_\\theta(x) - y\\right)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde\n",
        "\n",
        "$$  h_\\theta(x) - y =\n",
        "\\begin{bmatrix}\n",
        "h_\\theta\\left(x^{(1)}\\right) - y^{(1)} \\\\\n",
        "h_\\theta\\left(x^{(2)}\\right) - y^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "h_\\theta\\left(x^{(m)}\\right) - y^{(m)}\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "Nota $x^{(i)}$ es un vector, mientras $h_\\theta\\left(x^{(i)}\\right) - y^{(i)}$ es un escalar(simple n√∫mero).\n",
        "Para comprender el √∫ltimo paso de la derivaci√≥n, dejemos $\\beta_i = (h_\\theta\\left(x^{(m)}\\right) - y^{(m)})$ y\n",
        "observar que:\n",
        "\n",
        "$$ \\sum_i \\beta_ix^{(i)} = \\begin{bmatrix}\n",
        "| & | & & | \\\\\n",
        "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
        "| & | & & |\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\beta_1 \\\\\n",
        "\\beta_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_m\n",
        "\\end{bmatrix} = x^T \\beta\n",
        "$$\n",
        "\n",
        "donde los valores $\\beta_i = \\left( h_\\theta(x^{(i)} - y^{(i)} \\right)$.\n",
        "\n",
        "La expresi√≥n anterior nos permite calcular todas las derivadas parciales sin bucles.\n",
        "Si se siente c√≥modo con el √°lgebra lineal, le recomendamos que trabaje con las multiplicaciones de matrices anteriores para convencerse de que la versi√≥n vectorizada hace los mismos c√°lculos.\n",
        "\n",
        "<div class=\"alert alert-box alert-warning\">\n",
        "** Consejo de depuraci√≥n: ** El c√≥digo de vectorizaci√≥n a veces puede ser complicado. Una estrategia com√∫n para la depuraci√≥n es imprimir los tama√±os de las matrices con las que est√° trabajando usando la propiedad `shape` de las matrices` numpy`.\n",
        "\n",
        "Por ejemplo, dada una matriz de datos $X$ de tama√±o $100\\veces 20$ (100 ejemplos, 20 caracter√≠sticas) y $\\theta$, un vector con tama√±o $20$, puede observar que `np.dot (X, theta) `es una operaci√≥n de multiplicaci√≥n v√°lida, mientras que` np.dot (theta, X) `no lo es.\n",
        "\n",
        "Adem√°s, si tiene una versi√≥n no vectorizada de su c√≥digo, puede comparar la salida de su c√≥digo vectorizado y el c√≥digo no vectorizado para asegurarse de que produzcan las mismas salidas.</div>\n",
        "<a id=\"lrCostFunction\"></a>"
      ],
      "metadata": {
        "id": "IM-0FVcC8HFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Calcula la sigmoide de z.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ],
      "metadata": {
        "id": "oe5w6DxC8Hd4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcularCosto(theta, X, y):\n",
        "    # Inicializar algunos valores utiles\n",
        "    m = y.size  # numero de ejemplos de entrenamiento\n",
        "\n",
        "    J = 0\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
        "\n",
        "    return J"
      ],
      "metadata": {
        "id": "EiSoO84E8GY6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def descensoGradiente(theta, X, y, alpha, num_iters):\n",
        "    # Inicializa algunos valores\n",
        "    m = y.shape[0] # numero de ejemplos de entrenamiento\n",
        "\n",
        "    # realiza una copia de theta, el cual ser√° acutalizada por el descenso por el gradiente\n",
        "    theta = theta.copy()\n",
        "    J_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = sigmoid(X.dot(theta.T))\n",
        "        theta = theta - (alpha / m) * (h - y).dot(X)\n",
        "\n",
        "        J_history.append(calcularCosto(theta, X, y))\n",
        "    return theta, J_history"
      ],
      "metadata": {
        "id": "F_MI0rrG8Vei"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lrCostFunction(theta, X, y, lambda_):\n",
        "    \"\"\"\n",
        "    Calcula el costo de usar theta como par√°metro para la regresi√≥n log√≠stica regularizada y\n",
        "    el gradiente del costo w.r.t. a los par√°metros.\n",
        "\n",
        "    Parametros\n",
        "    ----------\n",
        "    theta : array_like\n",
        "        Parametro theta de la regresion logistica. Vector de la forma(shape) (n, ). n es el numero de caracteristicas\n",
        "        incluida la intercepcion\n",
        "\n",
        "    X : array_like\n",
        "        Dataset con la forma(shape) (m x n). m es el numero de ejemplos, y n es el numero de\n",
        "        caracteristicas (incluida la intercepcion).\n",
        "\n",
        "    y : array_like\n",
        "        El conjunto de etiquetas. Un vector con la forma (shape) (m, ). m es el numero de ejemplos\n",
        "\n",
        "    lambda_ : float\n",
        "        Parametro de regularizaci√≥n.\n",
        "\n",
        "    Devuelve\n",
        "    -------\n",
        "    J : float\n",
        "        El valor calculado para la funcion de costo regularizada.\n",
        "\n",
        "    grad : array_like\n",
        "        Un vector de la forma (shape) (n, ) que es el gradiente de la\n",
        "        funci√≥n de costo con respecto a theta, en los valores actuales de theta..\n",
        "    \"\"\"\n",
        "#     alpha = 0.003\n",
        "#     theta = theta.copy()\n",
        "    # Inicializa algunos valores utiles\n",
        "    m = y.size\n",
        "\n",
        "    # convierte las etiquetas a valores enteros si son boleanos\n",
        "    if y.dtype == bool:\n",
        "        y = y.astype(int)\n",
        "\n",
        "    J = 0\n",
        "    grad = np.zeros(theta.shape)\n",
        "\n",
        "    h = sigmoid(X.dot(theta.T))\n",
        "\n",
        "    temp = theta\n",
        "    temp[0] = 0\n",
        "\n",
        "#     J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
        "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n",
        "\n",
        "    grad = (1 / m) * (h - y).dot(X)\n",
        "#     theta = theta - (alpha / m) * (h - y).dot(X)\n",
        "    grad = grad + (lambda_ / m) * temp\n",
        "\n",
        "    return J, grad\n",
        "#    return J, theta"
      ],
      "metadata": {
        "id": "JHf24j0f8aO7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.3 Vectorizaci√≥n regularizada de la regresi√≥n log√≠stica\n",
        "\n",
        "Una vez implementada la vectorizaci√≥n para la regresi√≥n log√≠stica, corresponde agregarar regularizaci√≥n a la funci√≥n de costo.\n",
        "Para la regresi√≥n log√≠stica regularizada, la funci√≥n de costo se define como\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left(h_\\theta\\left(x^{(i)} \\right)\\right) - \\left( 1 - y^{(i)} \\right) \\log\\left(1 - h_\\theta \\left(x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
        "\n",
        "Tomar en cuenta que no deber√≠a regularizarse $\\theta_0$ que se usa para el t√©rmino de sesgo. En consecuencia, la derivada parcial del costo de regresi√≥n log√≠stica regularizado para $\\theta_j$ se define como\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}  & \\text{for } j = 0 \\\\\n",
        "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\text{for } j  \\ge 1\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "<div class=\"alert alert-box alert-warning\">\n",
        "** Python/numpy Consejo: ** Al implementar la vectorizaci√≥n para la regresi√≥n log√≠stica regularizada, a menudo es posible que solo desee sumar y actualizar ciertos elementos de $\\theta$. En `numpy`, puede indexar en las matrices para acceder y actualizar solo ciertos elementos.\n",
        "\n",
        "Por ejemplo, A [:, 3: 5] = B [:, 1: 3] reemplazar√° las columnas con √≠ndice 3 a 5 de A con las columnas con √≠ndice 1 a 3 de B.   \n",
        "Para seleccionar columnas (o filas) hasta el final de la matriz, puede dejar el lado derecho de los dos puntos en blanco.\n",
        "Por ejemplo, A [:, 2:] solo devolver√° elementos desde $3^{rd}$ a las √∫ltimas columnas de $A$.Si deja el tama√±o de la mano izquierda de los dos puntos en blanco, seleccionar√° los elementos del principio de la matriz.\n",
        "Por ejemplo, A [:,: 2] selecciona las dos primeras columnas y es equivalente a A [:, 0: 2]. Adem√°s, puede utilizar √≠ndices negativos para indexar matrices desde el final.\n",
        "Por lo tanto, A [:,: -1] selecciona todas las columnas de A excepto la √∫ltima columna, y A [:, -5:] selecciona la columna $5^{th}$ desde el final hasta la √∫ltima columna.\n",
        "\n",
        "Por lo tanto, podr√≠a usar esto junto con las operaciones de suma y potencia ($^{**}$) para calcular la suma de solo los elementos que le interesan (por ejemplo, `np.sum (z[1:]**2)`).\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "4FcoYvaQA48R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 Clasificacion One-vs-all\n",
        "\n",
        "En esta parte del ejercicio, se implementar√° la clasificaci√≥n de uno contra todos mediante el entrenamiento de m√∫ltiples clasificadores de regresi√≥n log√≠stica regularizados, uno para cada una de las clases  ùêæ  en nuestro conjunto de datos. En el conjunto de datos de d√≠gitos escritos a mano,  ùêæ=10 , pero su c√≥digo deber√≠a funcionar para cualquier valor de  ùêæ .\n",
        "\n",
        "El argumento y de esta funci√≥n es un vector de etiquetas de 0 a 9. Al entrenar el clasificador para la clase  ùëò‚àà{0,...,ùêæ‚àí1} , querr√° un vector K-dimensional de etiquetas  ùë¶ , donde  ùë¶ùëó ùëñùëõ0,1  indica si la instancia de entrenamiento  ùëóùë°‚Ñé  pertenece a la clase  ùëò   (ùë¶ùëó=1) , o si pertenece a una clase diferente  (ùë¶ùëó=0) .\n",
        "\n",
        "Adem√°s, se utiliza optimize.minimize de scipy para este ejercicio."
      ],
      "metadata": {
        "id": "LWh3F3o39KJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 80% entrenamiento, 20% prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Conjunto de entrenamiento:\", X_train.shape)\n",
        "print(\"Conjunto de prueba:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVY6WmqDDnnV",
        "outputId": "6eb9bceb-e032-4915-f7f9-0ab9acb687c9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conjunto de entrenamiento: (80, 16)\n",
            "Conjunto de prueba: (21, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 7\n",
        "lambda_ = 0.1\n",
        "\n",
        "all_theta = OneVsAllOM(X_train, y_train, num_labels, lambda_)"
      ],
      "metadata": {
        "id": "4YZWTcKUDsaz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precisi√≥n en entrenamiento\n",
        "y_pred_train = predictOneVsAll(all_theta, X_train)\n",
        "print(\"Precisi√≥n en entrenamiento: {:.2f}%\".format(np.mean(y_pred_train == y_train) * 100))\n",
        "\n",
        "# Precisi√≥n en prueba\n",
        "y_pred_test = predictOneVsAll(all_theta, X_test)\n",
        "print(\"Precisi√≥n en prueba: {:.2f}%\".format(np.mean(y_pred_test == y_test) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JPOBCQcDxxA",
        "outputId": "b9023406-9052-4f54-ae23-0bd23ef1430c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n en entrenamiento: 100.00%\n",
            "Precisi√≥n en prueba: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def OneVsAll(X, y, num_labels, lambda_=0):\n",
        "    alpha = 0.01\n",
        "    num_iters = 3000  # menos iteraciones, m√°s pr√°ctico en zoo (101 ejemplos)\n",
        "\n",
        "    m, n = X.shape\n",
        "    all_theta = np.zeros((num_labels, n + 1))\n",
        "\n",
        "    # Agregar columna de unos a X\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    for c in range(num_labels):\n",
        "        initial_theta = np.zeros(n + 1)\n",
        "\n",
        "        # Etiquetas binarias: 1 si clase == c, 0 en caso contrario\n",
        "        y_actual = np.where(y == c, 1, 0)\n",
        "\n",
        "        theta, J_history = descensoGradiente(initial_theta, X, y_actual,\n",
        "                                             alpha, num_iters, lambda_)\n",
        "\n",
        "        all_theta[c] = theta\n",
        "\n",
        "        # Opcional: graficar convergencia\n",
        "        plt.plot(np.arange(len(J_history)), J_history, lw=2, label=f\"Clase {c}\")\n",
        "\n",
        "    plt.xlabel('Iteraciones')\n",
        "    plt.ylabel('Costo J')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return all_theta\n",
        ""
      ],
      "metadata": {
        "id": "MjcQ3r369Kfo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import optimize\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def lrCostFunction(theta, X, y, lambda_):\n",
        "    \"\"\"\n",
        "    Funci√≥n de costo regularizada para regresi√≥n log√≠stica.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    h = sigmoid(X.dot(theta))\n",
        "\n",
        "    # costo\n",
        "    J = (-1/m) * (y.T.dot(np.log(h)) + (1-y).T.dot(np.log(1-h)))\n",
        "    J += (lambda_/(2*m)) * np.sum(np.square(theta[1:]))\n",
        "\n",
        "    # gradiente\n",
        "    grad = (1/m) * (X.T.dot(h - y))\n",
        "    grad[1:] += (lambda_/m) * theta[1:]\n",
        "\n",
        "    return J, grad\n",
        "\n",
        "\n",
        "def OneVsAllOM(X, y, num_labels, lambda_):\n",
        "    \"\"\"\n",
        "    Entrena num_labels clasificadores log√≠sticos y retorna\n",
        "    la matriz all_theta con cada clasificador en una fila.\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    all_theta = np.zeros((num_labels, n + 1))\n",
        "\n",
        "    # Agregar columna de unos (bias)\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    for c in np.arange(num_labels):\n",
        "        initial_theta = np.zeros(n + 1)\n",
        "        options = {'maxiter': 50}\n",
        "\n",
        "        res = optimize.minimize(lrCostFunction,\n",
        "                                initial_theta,\n",
        "                                args=(X, (y == c).astype(int), lambda_),\n",
        "                                jac=True,\n",
        "                                method='CG',\n",
        "                                options=options)\n",
        "\n",
        "        all_theta[c] = res.x\n",
        "\n",
        "    return all_theta"
      ],
      "metadata": {
        "id": "wUNxNDxA9JzJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularizaci√≥n\n",
        "lambda_ = 0.1\n",
        "\n",
        "# Entrenar One-vs-All con optimizaci√≥n\n",
        "all_theta = OneVsAllOM(X, y, num_labels, lambda_)\n",
        "\n",
        "# Mostrar dimensiones de la matriz de par√°metros\n",
        "print(all_theta.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2DxQrLW-zaR",
        "outputId": "448f0ba6-9537-4561-e750-4f67b4660b0b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6WuoKjH-2k1",
        "outputId": "afd1135f-d77b-4d34-f8bb-0777a9b8e264"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.48436660e+00  1.51562269e+00 -5.36634974e-01 -1.52249261e+00\n",
            "   2.29657787e+00 -2.53161267e-01  5.73916883e-02  7.69750275e-02\n",
            "   4.23983899e-01  4.71943834e-01  7.73444748e-01 -4.75189016e-01\n",
            "   2.46812659e-02  4.43840337e-02  2.91057337e-01  1.49260772e-01\n",
            "   7.20096306e-01]\n",
            " [-4.86562173e+00 -4.92065596e-01  2.81067076e+00  3.40172676e-01\n",
            "  -4.40272600e-01  7.56529991e-01 -8.78863783e-03  1.34124028e-01\n",
            "  -1.06861978e+00  6.50029751e-01  4.37792778e-01 -2.68475843e-01\n",
            "  -4.12862127e-01 -4.96119117e-01  6.17191396e-01  1.12915629e-01\n",
            "   9.43154998e-02]\n",
            " [-6.58180424e+00 -1.76911474e+00 -2.39922526e+00  2.47820279e-01\n",
            "  -1.78862507e+00 -8.28311503e-01 -1.96355073e+00 -2.10562183e-01\n",
            "   2.17629932e-01  1.84141365e+00 -4.80069579e-01  6.07887144e-01\n",
            "  -2.19462849e+00 -7.18937694e-01  1.51520883e+00 -3.86257520e-01\n",
            "  -2.86452955e-02]\n",
            " [-7.37730182e+00 -1.93229170e-01 -2.52579367e-01  1.33667309e+00\n",
            "  -9.00193449e-01 -1.84135012e-01  4.21861179e-01 -2.43796784e-01\n",
            "   6.59806392e-01  5.62355465e-01 -1.72196509e+00 -2.52214007e-01\n",
            "   1.84392295e+00 -6.25797878e-01  6.18653680e-01  8.13359689e-02\n",
            "  -8.36332431e-03]\n",
            " [-7.26483747e+00 -6.82464116e-01 -6.82396480e-01  7.65919103e-01\n",
            "  -7.13763883e-01 -5.92709104e-01  2.19226618e+00  3.59822717e-02\n",
            "   1.45544399e+00  1.03443759e+00  1.49800977e+00  7.03281509e-02\n",
            "  -7.80949488e-01  1.00932058e+00 -7.57323644e-01 -1.66387221e-01\n",
            "  -8.19197619e-01]\n",
            " [-9.12502365e+00  2.97550379e-01 -5.00483315e-01  9.11041008e-01\n",
            "  -2.67021504e-01  1.09108117e+00 -1.14896112e+00 -9.94279453e-01\n",
            "  -3.98882902e-01 -1.07804562e+00  1.20314569e+00 -3.56348491e-01\n",
            "  -4.71518727e-03  2.07411517e+00 -1.34286571e+00 -7.25436640e-02\n",
            "  -5.89560077e-01]\n",
            " [-8.15816548e+00 -3.88814230e-01 -4.07965639e-01 -6.42799227e-01\n",
            "  -1.18081487e-01 -1.63825333e+00  3.45975363e-01  1.40200018e+00\n",
            "  -2.10844537e+00 -3.18491964e+00 -7.88197033e-01  4.38941878e-01\n",
            "  -5.92266856e-01 -1.36945448e+00 -1.83448932e-01 -1.50973860e-01\n",
            "   3.44443093e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4.1 Prediccion One-vs-all\n",
        "\n",
        "Despu√©s de entrenar el clasificador de one-vs-all, se puede usarlo para predecir el d√≠gito contenido en una imagen determinada. Para cada entrada, debe calcular la \"probabilidad\" de que pertenezca a cada clase utilizando los clasificadores de regresi√≥n log√≠stica entrenados. La funci√≥n de predicci√≥n one-vs-all seleccionar√° la clase para la cual el clasificador de regresi√≥n log√≠stica correspondiente genera la probabilidad m√°s alta y devolver√° la etiqueta de clase (0, 1, ..., K-1) como la predicci√≥n para el ejemplo de entrada."
      ],
      "metadata": {
        "id": "vGOpPzUx_HiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictOneVsAll(all_theta, X):\n",
        "    \"\"\"\n",
        "    Devuelve un vector de predicciones para cada ejemplo en la matriz X.\n",
        "    - all_theta: matriz (K x n+1) con los par√°metros entrenados.\n",
        "    - X: matriz (m x n) con las caracter√≠sticas (sin el bias).\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    num_labels = all_theta.shape[0]\n",
        "\n",
        "    # Add ones to the X data matrix\n",
        "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    # Probabilidades para cada clase\n",
        "    probs = sigmoid(X.dot(all_theta.T))\n",
        "\n",
        "    # Predicci√≥n = √≠ndice de la probabilidad mayor\n",
        "    p = np.argmax(probs, axis=1)\n",
        "\n",
        "    return p"
      ],
      "metadata": {
        "id": "9Cw72YqB_H8B"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez que haya terminado, se llama a la funci√≥n predictOneVsAll usando el valor aprendido de  ùúÉ . Deber√≠a apreciarse que la precisi√≥n del conjunto de entrenamiento es de aproximadamente 95,1% (es decir, clasifica correctamente el 95,1% de los ejemplos del conjunto de entrenamiento)."
      ],
      "metadata": {
        "id": "I01mAHVD9JVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)  # (101, 16)\n",
        "\n",
        "pred = predictOneVsAll(all_theta, X)\n",
        "print('Precisi√≥n del conjunto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))\n",
        "\n",
        "# Tomamos el √∫ltimo animal como prueba\n",
        "XPrueba = X[100:101, :].copy()\n",
        "print(XPrueba.shape)  # (1, 16)\n",
        "\n",
        "# Agregar bias\n",
        "XPrueba = np.concatenate([np.ones((XPrueba.shape[0], 1)), XPrueba], axis=1)\n",
        "print(XPrueba.shape)  # (1, 17)\n",
        "\n",
        "# Predicci√≥n\n",
        "p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis=1)\n",
        "print(\"Predicci√≥n:\", p)\n",
        "\n",
        "# Clase real\n",
        "print(\"Etiqueta real:\", y[100:101])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ymvvt0L_rht",
        "outputId": "02b69b55-dd6c-4664-a519-616dba1fc85b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 16)\n",
            "Precisi√≥n del conjunto de entrenamiento: 100.00%\n",
            "(1, 16)\n",
            "(1, 17)\n",
            "Predicci√≥n: [1]\n",
            "Etiqueta real: [1]\n"
          ]
        }
      ]
    }
  ]
}